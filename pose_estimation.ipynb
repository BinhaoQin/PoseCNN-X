{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"DDJwQPZcupab"},"source":["# ROB 498-002/599-009 Final Project: Extending PoseCNN\n","\n","Binhao QIN, Peter MNEV"]},{"cell_type":"markdown","metadata":{"id":"LfBk3NtRgqaV"},"source":["# Getting Started"]},{"cell_type":"markdown","metadata":{"id":"ubB_0e-UAOVK"},"source":["## Setup Code\n","Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASkY27ZtA7Is"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"MzqbYcKdz6ew"},"source":["### Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19486,"status":"ok","timestamp":1682388078578,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"HzRdJ3uhe1CR","outputId":"541f4d36-778f-4783-ab4f-b4660fab8ede"},"outputs":[],"source":["is_colab_env = True\n","\n","try:\n","    from google.colab import drive\n","except ModuleNotFoundError as _:\n","    is_colab_env = False\n","\n","if is_colab_env:\n","    drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"OvUDZWGU3VLV"},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the following cell should print the filenames from the assignment:\n","\n","```\n","[\"p4_helper.py\", \"rob599\", \"pose_cnn.py\", \"pose_estimation.ipynb\"]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682370061204,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"RrAX9FOLpr9k","outputId":"f4a7eb92-ec0f-4a7a-b9ff-39b027da11a9"},"outputs":[],"source":["import os\n","import sys\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 2023WN folder and put all the files under P4 folder, then \"2023WN/P4\"\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2023WN/P4'\n","if is_colab_env:\n","    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/ROB498-WN23/projects/final'\n","    GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","else:\n","    GOOGLE_DRIVE_PATH = os.path.curdir\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"XkhvDeZFGyNB"},"source":["Next, we install a couple packages to help with processing and visualizing object models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23723,"status":"ok","timestamp":1682370084923,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"76xg6TA-GSAZ","outputId":"4b209b40-ef50-4b91-d609-1aa755b8913a"},"outputs":[],"source":["import os\n","os.environ['PYOPENGL_PLATFORM'] = 'egl'\n","\n","%pip install trimesh\n","%pip install pyrender\n","%pip install pyquaternion"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RldDumJE48pv"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this project. If it works correctly, it should print the last edit time for the file `pose_cnn.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2988,"status":"ok","timestamp":1682370087907,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"pTIwSpkS495_","outputId":"372280c5-3b9a-4a06-ab6c-1fe0b31ebde8"},"outputs":[],"source":["import os\n","import time\n","\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","pose_cnn_path = os.path.join(GOOGLE_DRIVE_PATH, \"pose_cnn.py\")\n","pose_cnn_edit_time = time.ctime(\n","    os.path.getmtime(pose_cnn_path)\n",")\n","print(\"pose_cnn.py last edited on %s\" % pose_cnn_edit_time)"]},{"cell_type":"markdown","metadata":{"id":"GWP1vCGL5Eca"},"source":["Load several useful packages that are used in this notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwVZ26yM5G8U"},"outputs":[],"source":["import os\n","import time\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","%matplotlib inline\n","\n","from utils import *\n","from rob599 import reset_seed\n","from rob599.grad import rel_error\n","\n","# for plotting\n","plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n","plt.rcParams[\"font.size\"] = 16\n","plt.rcParams[\"image.interpolation\"] = \"nearest\"\n","plt.rcParams[\"image.cmap\"] = \"gray\""]},{"cell_type":"markdown","metadata":{"id":"x7poKGI35JZY"},"source":["We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682370087908,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"Vw3wIuCu5LnU","outputId":"9331d3cd-9208-4f6b-cdce-3e220d386ae6"},"outputs":[],"source":["if torch.cuda.is_available():\n","    print(\"Good to go!\")\n","    DEVICE = torch.device(\"cuda\")\n","else:\n","    print(\"Please set GPU via Edit -> Notebook Settings.\")\n","    DEVICE = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"MjJ3uyYBg3Lw"},"source":["## Load PROPS Pose Dataset\n","During the majority of our homework assignments so far, we have used the PROPS Classification or Detection datasets for image processing tasks.\n","\n","In order to train and evaluate object pose estimation models, we need a dataset where each image is annotated with a *set* of *pose labels*, where each pose label gives the 3DoF position and 3DoF orientation of some object in the image.\n","\n","We will use the [PROPS Pose](https://deeprob.org/datasets/props-pose/) dataset, which provides annotations of this form. \n","Our PROPS Detection dataset is much smaller than typical benchmarking pose estimation datasets, and thus easier to manage in an homework assignment.\n","PROPS comprises annotated bounding boxes for 10 object classes:\n","`[\"master_chef_can\", \"cracker_box\", \"sugar_box\", \"tomato_soup_can\", \"mustard_bottle\", \"tuna_fish_can\", \"gelatin_box\", \"potted_meat_can\", \"mug\", \"large_marker\"]`.\n","The choice of these objects is inspired by the [YCB object and Model set](https://ieeexplore.ieee.org/document/7251504) commonly used in robotic perception models.\n","\n","We create a [`PyTorch Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class\n","named `PROPSPoseDataset` in `rob599/PROPSPoseDataset.py` that will download the PROPS Pose dataset.\n","\n","Run the following two cells to set a few config parameters and then download the train/val sets for the PROPS Pose dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AetPkU4wEmlm"},"outputs":[],"source":["import multiprocessing\n","\n","# Set a few constants related to data loading.\n","NUM_CLASSES = 10\n","BATCH_SIZE = 4\n","NUM_WORKERS = multiprocessing.cpu_count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38192,"status":"ok","timestamp":1682370126781,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"BJe14F-D5Cgk","outputId":"5d7c2f2f-ead6-4d1e-8668-cee35ae2a4ce"},"outputs":[],"source":["from rob599 import PROPSPoseDataset\n"," \n","# NOTE: Set `download=True` for the first time when you set up Google Drive folder.\n","# Turn it back to `False` later for faster execution in the future.\n","# If this hangs, download and place data in your drive manually.\n","train_dataset = PROPSPoseDataset(\n","    GOOGLE_DRIVE_PATH, \"train\",\n","    download=False  # True (for the first time)\n",") \n","val_dataset = PROPSPoseDataset(GOOGLE_DRIVE_PATH, \"val\")\n","\n","print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"]},{"cell_type":"markdown","metadata":{"id":"MlbWZBvCvCz_"},"source":["This dataset will format each sample from the dataset as a dictionary containing the following keys:\n","\n"," - 'rgb': a numpy float32 array of shape (3, 480, 640) scaled to range [0,1]\n"," - 'depth': a numpy int32 array of shape (1, 480, 640) in (mm)\n"," - 'objs_id': a numpy uint8 array of shape (10,) containing integer ids for visible objects (1-10) and invisible objects (0)\n"," - 'label': a numpy bool array of shape (11, 480, 640) containing instance segmentation for objects in the scene\n"," - 'bbx': a numpy float64 array of shape (10, 4) containing (x, y, w, h) coordinates of object bounding boxes\n"," - 'RTs': a numpy float64 array of shape (10, 3, 4) containing homogeneous transformation matrices per object into camera coordinate frame\n"," - 'centermaps': a numpy float64 array of shape (30, 480, 640) containing (dx, dy, z) coordinates to each object's centroid \n"," - 'centers': a numpy float64 array of shape (10, 2) containing (x, y) coordinates of object centroids projected to image plane \n"," \n","This dataset assumes that the upper left of the image is the origin point (0, 0)."]},{"cell_type":"markdown","metadata":{"id":"KOahI5F1vCz_"},"source":["### Visualize Dataset\n","\n","Now let's visualize a few samples from our validation set to make sure the images and labels are loaded correctly. In this next cell, we'll use the `visualize_dataset` function from `rob599/utils.py` to view the RGB observation and labeled pose labels for three random samples. \n","\n","In the below figure, the final column plots the centermaps for class 0, which corresponds to the master chef coffee can. This plot is included to give a sense of how the centermaps represent gradients towards the object's centroid."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"elapsed":6249,"status":"ok","timestamp":1679781167245,"user":{"displayName":"Binhao Qin","userId":"14278190121126910377"},"user_tz":240},"id":"A756xlwsvCz_","outputId":"19f41ed4-3f8a-4ada-90a8-858e175e0b67","scrolled":false},"outputs":[],"source":["from rob599 import reset_seed, visualize_dataset\n","\n","reset_seed(0)\n","\n","grid_vis = visualize_dataset(val_dataset,alpha = 0.25)\n","plt.axis('off')\n","plt.imshow(grid_vis)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"C22eTf2qvC0A"},"source":["## Extending PoseCNN\n","\n","Now that we have our dataset loaded and ready to use, we'll begin implementing a variant of the [PoseCNN](https://arxiv.org/abs/1711.00199) network. This architecture is designed to take an RGB color image as input and produce a [6 degrees-of-freedom pose](https://en.wikipedia.org/wiki/Six_degrees_of_freedom) estimate for each instance of an object within the scene from which the image was taken. To do this, PoseCNN uses 5 operations within the architecture. First, a backbone convolutional feature extraction network is used to produce a tensor representing learned features from the input image. Second, the extracted features are processed by an embedding branch to reduce the spatial resolution and memory overhead for downstream layers. Third, an instance segmentation branch uses the embedded features to identify regions in the image corresponding to each object instance (regions of interest). Fourth, the translations for each object instance are estimated using a translation branch along with the embedded features. Finally, a rotation branch uses the embedded features to estimate a rotation, in the form of a [quaternion](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation), for each region of interest.\n","\n","Thr architecture is shown in more detail from Figure 2 of the [PoseCNN paper](https://arxiv.org/abs/1711.00199):\n","\n","![architecture](https://deeprob.org/assets/images/posecnn_arch.png)\n","\n","Now, we will implement a variant of this architecture which uses the ResNet50 with FPN as the backbone feature extractor and individual branches that perform operations based on FPN features using PyTorch and data from our `PROPSPoseDataset`. The remainder of the features for this project will be implemented in the `pose_cnn.py` file."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_QUyBVaOvC0B"},"source":["## Implementing Segmentation Branch\n","\n","Now that we have our feature extractor setup, we'll implement the instnace segmentation branch. This branch should fuse information from the feature extractor according to the architecture diagram of PoseCNN. Specifically, the network will pass output from the feature extractor through a 1x1 convolution+ReLU layer followed by interpolation and an element wise addition. Next, the intermediate feature is interpolated back to the input image size followed by a final 1x1 convolution+ReLU layer to predict a probability for each class or background at each pixel."]},{"cell_type":"markdown","metadata":{"id":"yzWTFjipvC0B"},"source":["### Training PoseCNN to Perform Instance Segmentation\n","\n","Once you've added code to initialize and perform the forward pass of PoseCNN for feature extraction and instance segmentation, we can attempt to train this part of PoseCNN by itself. The code in the following cell will initialize a PoseCNN model and begin training it on instance segmentation only. You should expect to see your training loss decrease to ~0.1 after training for 2 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":791},"executionInfo":{"elapsed":664886,"status":"ok","timestamp":1682369659922,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"W2ckF-yivC0C","outputId":"7331daf6-51a6-4d90-dc14-6cdca2a08b01"},"outputs":[],"source":["import time\n","from torch.utils.data import DataLoader\n","import torchvision.models as models\n","\n","from rob599 import reset_seed\n","from pose_cnn import PoseCNN\n","from tqdm import tqdm\n","\n","reset_seed(0)\n","\n","posecnn_model = PoseCNN(\n","                       models_pcd = torch.tensor(train_dataset.models_pcd).to(DEVICE, dtype=torch.float32),\n","                       cam_intrinsic = train_dataset.cam_intrinsic).to(DEVICE)\n","\n","dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n","optimizer = torch.optim.Adam(posecnn_model.parameters(), lr=0.001,\n","                                 betas=(0.9, 0.999))\n","\n","posecnn_model.train()\n","\n","loss_history = []\n","log_period = 5\n","_iter = 0\n","\n","st_time = time.time()\n","for epoch in range(3):\n","    train_loss = []\n","    for batch in tqdm(dataloader):\n","        for item in batch:\n","            batch[item] = batch[item].to(DEVICE)\n","        loss_dict = posecnn_model(batch)\n","        optimizer.zero_grad()\n","        total_loss = loss_dict[\"loss_segmentation\"]\n","        total_loss.backward()\n","        optimizer.step()\n","        train_loss.append(total_loss.item())\n","    \n","        if _iter % log_period == 0:\n","            loss_history.append(total_loss.item())\n","        _iter += 1\n","    \n","    print('Time {0}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)) + \\\n","                                  ', ' + 'Epoch %02d' % epoch + ', ' + 'Training finished' + f' , with mean training loss {np.array(train_loss).mean()}'))\n","    \n","plt.title(\"Training loss history\")\n","plt.xlabel(f\"Iteration (x {log_period})\")\n","plt.ylabel(\"Loss\")\n","plt.plot(loss_history)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QdY9u21XvC0C"},"source":["### Inference for Instance Segmentation\n","\n","Now that we have our segmentation network trained, we can qualitatively evaluate the segmentation results. The following notebook cell will visualize output segmentations on a sample from the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":674},"executionInfo":{"elapsed":5857,"status":"ok","timestamp":1682381722653,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"-QeMzU8tvC0C","outputId":"31c3a072-9848-4483-e9c0-ad885cc5ba6c","scrolled":false},"outputs":[],"source":["from torchvision.utils import make_grid\n","\n","reset_seed(0)\n","\n","num_samples = 3\n","posecnn_model.eval()\n","\n","plt.text(300, -40, 'RGB', ha=\"center\")\n","plt.text(950, -40, 'True\\nSegmentation', ha=\"center\")\n","plt.text(1600, -40, 'Predicted\\nSegmentation', ha=\"center\")\n","\n","samples = []\n","for sample_i in range(num_samples):\n","    sample_idx = random.randint(0,len(val_dataset)-1)\n","    sample = val_dataset[sample_idx]\n","    \n","    rgb = torch.tensor(sample['rgb'][None, :]).to(DEVICE)\n","    _, prediction = posecnn_model({'rgb': rgb})\n","    prediction = prediction.cpu().numpy().astype(np.float64)\n","    prediction /= prediction.max()\n","    prediction = (np.tile(prediction, (3, 1, 1)) * 255).astype(np.uint8)\n","    rgb = (sample['rgb'].transpose(1, 2, 0) * 255).astype(np.uint8)\n","    depth = ((np.tile(sample['depth'], (3, 1, 1)) / sample['depth'].max()) * 255).astype(np.uint8)\n","    segmentation = (sample['label']*np.arange(11).reshape((11,1,1))).sum(0,keepdims=True).astype(np.float64)\n","    segmentation /= segmentation.max()\n","    segmentation = (np.tile(segmentation, (3, 1, 1)) * 255).astype(np.uint8)\n","    \n","    samples.append(torch.tensor(rgb.transpose(2, 0, 1)))\n","    samples.append(torch.tensor(segmentation))\n","    samples.append(torch.tensor(prediction))\n","\n","img = make_grid(samples, nrow=3).permute(1, 2, 0)\n","\n","plt.axis('off')\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2jI1YIY0vC0D"},"source":["Before moving on, visually inspect the segmentation results above to ensure your forward functions and loss calculations are setup correctly."]},{"cell_type":"markdown","metadata":{"id":"O8pNLMbEvC0E"},"source":["## Putting it all together: PoseCNN\n","\n","We now have all the modules needed to make up our PoseCNN architecture. In the `PoseCNN` class of `pose_cnn.py`, add the translation and rotation branches to the initialization and forward functions. During training, your PoseCNN model should output a `loss_dict` variable with loss values for segmentation, translation and rotation branches stored respectively with keys of `\"loss_segmentation\"`, `\"loss_centermap\"`, and `\"loss_R\"`. The segmentation loss should be calculated using `p4_helper.loss_cross_entropy`, the centroid loss should be calculated using l1Loss, and the rotation loss should be calculated using the provided helper in `p4_helper.loss_Rotation`. During inference, your model should output a dictionary of predicted poses (i.e. see `PoseCNN.generate_pose` for a formatting utility) in `output_dict` and the predicted segmentation map (post processed probabilities) in `segmentation`.\n","\n","After this, you will be ready to train your PoseCNN model with all three losses:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2297222,"status":"ok","timestamp":1682384143960,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"_7Pn98JavC0F","outputId":"79470a41-9d95-4337-f062-548ede0a975a","scrolled":false},"outputs":[],"source":["import os\n","import time\n","import torch\n","from torch.utils.data import DataLoader\n","import torchvision.models as models\n","\n","import rob599\n","from pose_cnn import PoseCNN\n","\n","rob599.reset_seed(0)\n","\n","dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n","\n","posecnn_model = PoseCNN(\n","                models_pcd = torch.tensor(train_dataset.models_pcd).to(DEVICE, dtype=torch.float32),\n","                cam_intrinsic = train_dataset.cam_intrinsic).to(DEVICE)\n","posecnn_model.train()\n","    \n","optimizer = torch.optim.Adam(posecnn_model.parameters(), lr=0.001,\n","                            betas=(0.9, 0.999))\n","\n","\n","loss_history = []\n","log_period = 5\n","_iter = 0\n","\n","\n","st_time = time.time()\n","for epoch in range(10):\n","    train_loss = []\n","    dataloader.dataset.dataset_type = 'train'\n","    for batch in dataloader:\n","        for item in batch:\n","            batch[item] = batch[item].to(DEVICE)\n","        loss_dict = posecnn_model(batch)\n","        optimizer.zero_grad()\n","        total_loss = 0\n","        for loss in loss_dict:\n","            total_loss += loss_dict[loss]\n","        total_loss.backward()\n","        optimizer.step()\n","        train_loss.append(total_loss.item())\n","        \n","        if _iter % log_period == 0:\n","            loss_str = f\"[Iter {_iter}][loss: {total_loss:.3f}]\"\n","            for key, value in loss_dict.items():\n","                loss_str += f\"[{key}: {value:.3f}]\"\n","\n","            print(loss_str)\n","            loss_history.append(total_loss.item())\n","        _iter += 1\n","        \n","    print('Time {0}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)) + \\\n","                                  ', ' + 'Epoch %02d' % epoch + ', ' + 'Training finished' + f' , with mean training loss {np.array(train_loss).mean()}'))    \n","\n","torch.save(posecnn_model.state_dict(), os.path.join(GOOGLE_DRIVE_PATH, \"posecnn_model.pth\"))\n","    \n","plt.title(\"Training loss history\")\n","plt.xlabel(f\"Iteration (x {log_period})\")\n","plt.ylabel(\"Loss\")\n","plt.plot(loss_history)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"KhWZT-ztEaqm"},"source":["### Inference\n","\n","Visualize a few outputs from the full trained model. These could be improved if we used a larger model, trained for greater duration, and if we used ICP with depth data to refine the final estimates."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":20106,"status":"ok","timestamp":1682384385375,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"J7ArGiLTnHta","outputId":"ec81d40d-d6a8-42b8-95d1-b1e4effe7bc7","scrolled":false},"outputs":[],"source":["import torch\n","import random\n","from torch.utils.data import DataLoader\n","import torchvision.models as models\n","\n","import rob599\n","from pose_cnn import PoseCNN, eval\n","\n","\n","rob599.reset_seed(0)\n","\n","dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)\n","\n","posecnn_model = PoseCNN(\n","                models_pcd = torch.tensor(val_dataset.models_pcd).to(DEVICE, dtype=torch.float32),\n","                cam_intrinsic = val_dataset.cam_intrinsic).to(DEVICE)\n","posecnn_model.load_state_dict(torch.load(os.path.join(GOOGLE_DRIVE_PATH, \"posecnn_model.pth\")))\n","\n","num_samples = 5\n","for i in range(num_samples):\n","    out = eval(posecnn_model, dataloader, DEVICE)\n","\n","    plt.axis('off')\n","    plt.imshow(out)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zBsbAC9lvC0F"},"source":["Finally, let's measure the quantitative accuracy of our trained model using the 5°5cm metric. That is, we'll count how many visible objects our model was able to predict correctly, where a correct prediction is defined as one with a rotation error of less than 5° and a translation error of less than 5cm.\n","\n","The instructor's model, trained with the hyperparameters above achieves 29.3%."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221322,"status":"ok","timestamp":1682384365276,"user":{"displayName":"秦彬皓","userId":"15057989239655692336"},"user_tz":240},"id":"bS20B-qjvC0F","outputId":"11eac624-9971-434f-cca8-cc2b27c21245","scrolled":true},"outputs":[],"source":["import math\n","import torch\n","from torch.utils.data import DataLoader\n","import torchvision.models as models\n","\n","import pyquaternion\n","from tqdm import tqdm\n","\n","import rob599\n","from pose_cnn import PoseCNN\n","\n","rob599.reset_seed(0)\n","\n","dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)\n","\n","posecnn_model.load_state_dict(torch.load(os.path.join(GOOGLE_DRIVE_PATH, \"posecnn_model.pth\")))\n","posecnn_model.eval()\n","\n","\n","T_thresh = 5 # cm\n","R_thresh = 5 # deg\n","\n","total =0\n","correct = 0\n","for batch in tqdm(dataloader):\n","    for item in batch:\n","        batch[item] = batch[item].to(DEVICE)\n","    pose_dict, segmentation = posecnn_model(batch)\n","    for bidx in range(BATCH_SIZE):\n","        objs_visib = batch['objs_id'][bidx].cpu().tolist()\n","        objs_preds = sorted(list(pose_dict[bidx].keys()))\n","        for objidx, objs_id in enumerate(objs_visib):\n","            if objs_id==0:\n","                continue\n","\n","            total += 1\n","            if objs_id not in objs_preds:\n","                continue\n","            RT_pred = pose_dict[bidx][objs_id]\n","            RT_true = batch['RTs'][bidx][objidx].cpu().numpy()\n","\n","            # Translation error\n","            T_pred = RT_pred[:3,3]\n","            T_true = RT_true[:3,3]\n","            T_err = 100*np.linalg.norm(T_pred-T_true) # error in cm\n","\n","            # Rotation error\n","            R_true = pyquaternion.Quaternion(matrix=RT_true[:3,:3],atol=1e-6)\n","            R_pred = pyquaternion.Quaternion(matrix=RT_pred[:3,:3],atol=1e-6)\n","\n","            R_rel = R_pred * R_true.conjugate\n","            R_err = math.degrees(R_rel.angle)\n","\n","            if T_err<T_thresh and R_err<R_thresh:\n","                correct+=1\n","\n","print(\"Accuracy at 5°5cm:\",correct/total)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xu7REXAFvC0G"},"source":["# Save Your Work\n","After completing this notebook, run the following cell to create a `.zip` file for you to download. \n","\n","**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98,"status":"ok","timestamp":1679799971210,"user":{"displayName":"Binhao Qin","userId":"14278190121126910377"},"user_tz":240},"id":"HglxsHApvC0G","outputId":"a187edbf-2d39-4fe4-f163-66621f12e032"},"outputs":[],"source":["from rob599.submit import make_p4_submission\n","\n","# TODO: Replace these with your actual uniquename and umid\n","uniquename = 'bhqin'\n","umid = 69209865\n","\n","make_p4_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1eb61ed168b04321b643797f1006a65c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_677b8b21219f4bc7b5b71347e1d1471c","placeholder":"​","style":"IPY_MODEL_ce84f9c11641476fa17d8d0baf5f2469","value":" 528M/528M [00:02&lt;00:00, 233MB/s]"}},"285255f4ebd34ff8905dff3b50eee8f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea53d235d56a4226abba66a2a93a0ef4","placeholder":"​","style":"IPY_MODEL_76d20b3648ea4be0a82eababb1248a52","value":"100%"}},"62b9bf55522a4052aec8331fde8274c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"677b8b21219f4bc7b5b71347e1d1471c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76d20b3648ea4be0a82eababb1248a52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b41eb93f1744e599dfc71d70ed65083":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5555c290ca943cea4baa7d2e39b880c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b41eb93f1744e599dfc71d70ed65083","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62b9bf55522a4052aec8331fde8274c1","value":553433881}},"c8b6468257584edf9b843a4a34c7ce56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce84f9c11641476fa17d8d0baf5f2469":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4bda03db6c045438e352ec07885d7ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_285255f4ebd34ff8905dff3b50eee8f2","IPY_MODEL_c5555c290ca943cea4baa7d2e39b880c","IPY_MODEL_1eb61ed168b04321b643797f1006a65c"],"layout":"IPY_MODEL_c8b6468257584edf9b843a4a34c7ce56"}},"ea53d235d56a4226abba66a2a93a0ef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
